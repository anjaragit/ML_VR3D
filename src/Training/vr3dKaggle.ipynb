{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow_addons as tfa\nimport argparse\nimport math\nimport h5py\nimport numpy as np\nimport tensorflow as tf\nfrom keras import backend as K\ntf.compat.v1.disable_eager_execution()\n#tf.compat.v1.enable_eager_execution()\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import f1_score\nimport socket\nimport importlib\nimport os\nimport sys\nimport datetime\nimport time \n\n\nBASE_DIR ='../input/vr3d-pts-bordeau'\nsys.path.append(BASE_DIR)\nsys.path.append(os.path.join(BASE_DIR, 'models'))\nsys.path.append(os.path.join(BASE_DIR, 'utils'))\nsys.path.append(os.path.join(BASE_DIR, 'logs'))\nimport tf_util\n\nMODEL = importlib.import_module('pointnet_seg') # import network module\nMAX_EPOCH = 21\nNUM_POINT = 4096  # help='Point Number [256/512/1024/2048/4096] [default: 1024]\nMAX_NUM_POINT = 4096\nBATCH_SIZE = 32\nNUM_CLASSES = 2 #2 #13 #40\n\nBASE_LEARNING_RATE = 0.001#0.001\nDECAY_STEP = 2000\nDECAY_RATE = 0.3 #0.3\n\ndef get_learning_rate_schedule():\n    learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(\n        BASE_LEARNING_RATE,  # Initial learning rate\n        DECAY_STEP,          # Decay step.\n        DECAY_RATE,          # Decay rate.\n        staircase=True)\n    return learning_rate\n\ndef Dice_loss(y_true,y_pred): \n    # y_true = tf.cast(y_true, tf.float32)\n    # y_pred = tf.math.sigmoid(y_pred)\n    numerator = 2 * tf.reduce_sum(y_true * y_pred)\n    denominator = tf.reduce_sum(y_true + y_pred)\n    return 1 - numerator / denominator\n\ndef DiceLoss(y_true, y_pred):\n        # y_true = tf.cast(y_true, tf.float32)\n    # y_pred = tf.math.sigmoid(y_pred)\n    numerator = 2 * K.sum(y_true * y_pred)\n    denominator = K.sum(y_true + y_pred)\n    return 1 - numerator / denominator\n    \n\ndef pixel_accuracy(y_true,y_pred):\n    tp = K.sum(K.round(K.clip(y_true*y_pred,0,1)))\n    tn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n    fp = K.sum(K.round(K.clip((1 - y_true) * y_pred, 0, 1)))\n    fn = K.sum(K.round(K.clip((y_true) * (1 - y_pred), 0, 1)))\n    return (tp )/(tp+fp)  #(tp )/(tp+fp) pourcentage de la valeur positive pr√®dit sont vraie\n\ndef recall(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall_keras = true_positives / (possible_positives + K.epsilon())\n    return recall_keras\n\ndef precision(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision_keras = true_positives / (predicted_positives + K.epsilon())\n    return precision_keras\n\ndef f1(y_true, y_pred):\n    p = precision(y_true, y_pred)\n    r = recall(y_true, y_pred)\n    return 2 * ((p * r) / (p + r + K.epsilon()))\n\ndef IOU(y_true,y_pred):\n    tp = K.sum(K.round(K.clip(y_true*y_pred,0,1)))\n    tn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n    fp = K.sum(K.round(K.clip((1 - y_true) * y_pred, 0, 1)))\n    fn = K.sum(K.round(K.clip((y_true) * (1 - y_pred), 0, 1)))\n    return 1 -(tp )/(tp+fp+fn) # retourner en tant que loss\n\ndef train():\n    with tf.device(\"gpu:0\"):\n            model = MODEL.get_model((None, 7), NUM_CLASSES)\n            model.summary()\n            learning_rate = get_learning_rate_schedule()\n            # optimizer = tf.keras.optimizers.Adam(0.01)\n            optimizer = tf.keras.optimizers.Adam(learning_rate)\n        \n            # initialize Dataset\n            PointCloudProvider.initialize_dataset()\n        \n            print('\\nput in generator...')\n            generator_training = PointCloudProvider('train', BATCH_SIZE, n_classes=NUM_CLASSES, sample_size=MAX_NUM_POINT)\n            generator_validation = PointCloudProvider('test', BATCH_SIZE, n_classes=NUM_CLASSES, sample_size=MAX_NUM_POINT)\n            print('\\ntraining...')\n            \n            log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y-%m-%d-%H:%M:%S\")\n            tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n            \n            loss = tf.keras.losses.sparse_categorical_crossentropy\n            model.compile(optimizer=optimizer,\n                  loss = loss,\n                  #loss = DiceLoss,\n                  metrics = [IOU,'accuracy']\n                  # metrics=[\"sparse_categorical_accuracy\"]\n                  #metrics=['accuracy']\n                            )\n\n            model.fit(generator_training, \n                      #validation_data=generator_validation,\n                      steps_per_epoch=len(generator_training),\n                      #validation_steps=len(generator_validation),\n                      epochs=MAX_EPOCH,use_multiprocessing=False,\n                      callbacks = [tensorboard_callback]\n                     )\n            \n            print('\\nsave model...')\n            model.save_weights(\"/kaggle/working/Trained_ep_21_vr3d-pts_bordeau_\"+datetime.datetime.now().strftime(\"%Y-%m-%d-%H:%M:%S\")+\"_Down_sample_bordeau_elevation_filtre.h5\")\n            print('\\nFinished...')\n            \n            \nif __name__ == \"__main__\":\n    train()","metadata":{"execution":{"iopub.status.busy":"2022-04-08T13:26:43.212357Z","iopub.execute_input":"2022-04-08T13:26:43.21272Z","iopub.status.idle":"2022-04-08T14:05:56.625555Z","shell.execute_reply.started":"2022-04-08T13:26:43.21269Z","shell.execute_reply":"2022-04-08T14:05:56.623918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Features Importantes","metadata":{}},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-02-08T10:51:45.375718Z","iopub.execute_input":"2022-02-08T10:51:45.376031Z","iopub.status.idle":"2022-02-08T10:52:59.947966Z","shell.execute_reply.started":"2022-02-08T10:51:45.376002Z","shell.execute_reply":"2022-02-08T10:52:59.946194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-02-03T06:18:20.04717Z","iopub.execute_input":"2022-02-03T06:18:20.047728Z","iopub.status.idle":"2022-02-03T06:27:17.886958Z","shell.execute_reply.started":"2022-02-03T06:18:20.047673Z","shell.execute_reply":"2022-02-03T06:27:17.885733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Zip log_file & download","metadata":{}},{"cell_type":"code","source":"import os\nfrom zipfile import ZipFile\nzipO = ZipFile('Tensorboard_total.zip','w')\nfor i in range(2,13,40):\n    for j in [20,28,32]:\n        zipObj = ZipFile('Tensorboard'+str(i)+'-'+str(j)+'.zip', 'w')\n        zipObj.write('./logs_'+str(i)+'-'+str(j))\n\n        for dirname, _, filenames in os.walk('./logs_'+str(i)+'-'+str(j)):\n            for filename in filenames:\n                zipObj.write(os.path.join(dirname, filename))        \n        zipObj.close()\n        zipO.write('Tensorboard'+str(i)+'-'+str(j)+'.zip')\nzipO.close()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom zipfile import ZipFile\nzipObj = ZipFile('Tensorboard_mergeMas.zip','w')\nzipObj.write('./logs')\nfor dirname, _, filenames in os.walk('./logs'):\n    for filename in filenames:\n        zipObj.write(os.path.join(dirname, filename))        \nzipObj.close()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T09:24:30.439339Z","iopub.execute_input":"2022-02-03T09:24:30.439711Z","iopub.status.idle":"2022-02-03T09:24:30.457645Z","shell.execute_reply.started":"2022-02-03T09:24:30.43966Z","shell.execute_reply":"2022-02-03T09:24:30.456885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom zipfile import ZipFile\nzipObj = ZipFile('tensorboard_pts.zip','w')\nzipObj.write('./logs')\nfor dirname, _, filenames in os.walk('./logs'):\n    for filename in filenames:\n        zipObj.write(os.path.join(dirname, filename))        \nzipObj.close()","metadata":{"execution":{"iopub.status.busy":"2022-04-07T10:52:16.95943Z","iopub.execute_input":"2022-04-07T10:52:16.95977Z","iopub.status.idle":"2022-04-07T10:52:17.03336Z","shell.execute_reply.started":"2022-04-07T10:52:16.959716Z","shell.execute_reply":"2022-04-07T10:52:17.03254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os,glob\nfrom zipfile import ZipFile\nzipObj = ZipFile('model_to2.zip','w')\n_path = './'\nos.chdir(_path)\nfor file in glob.glob('T*.h5'):\n    zipObj.write(file)        \nzipObj.close()","metadata":{"execution":{"iopub.status.busy":"2021-12-28T14:53:49.896206Z","iopub.execute_input":"2021-12-28T14:53:49.896561Z","iopub.status.idle":"2021-12-28T14:53:50.237457Z","shell.execute_reply.started":"2021-12-28T14:53:49.896526Z","shell.execute_reply":"2021-12-28T14:53:50.236481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! wget --no-check-certificate https://github.com/RobinBaumann/pointnet/blob/master/tf2/train.py","metadata":{"execution":{"iopub.status.busy":"2022-03-07T11:28:16.966761Z","iopub.execute_input":"2022-03-07T11:28:16.967204Z","iopub.status.idle":"2022-03-07T11:28:18.409652Z","shell.execute_reply.started":"2022-03-07T11:28:16.967107Z","shell.execute_reply":"2022-03-07T11:28:18.408637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! unzip shapenet_part_seg_hdf5_data.zip","metadata":{"execution":{"iopub.status.busy":"2022-03-07T11:28:50.313131Z","iopub.execute_input":"2022-03-07T11:28:50.313589Z","iopub.status.idle":"2022-03-07T11:28:50.968351Z","shell.execute_reply.started":"2022-03-07T11:28:50.313524Z","shell.execute_reply":"2022-03-07T11:28:50.967465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! zip  down-sample-bordeau-elevation-filtre.zip","metadata":{"execution":{"iopub.status.busy":"2022-01-28T08:54:53.332617Z","iopub.execute_input":"2022-01-28T08:54:53.333073Z","iopub.status.idle":"2022-01-28T08:54:54.103286Z","shell.execute_reply.started":"2022-01-28T08:54:53.332983Z","shell.execute_reply":"2022-01-28T08:54:54.102339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import h5py\npath = './hdf5_data/ply_data_train4.h5'\nfile_input = h5py.File(path,'r')\n","metadata":{"execution":{"iopub.status.busy":"2022-01-28T07:50:18.395745Z","iopub.execute_input":"2022-01-28T07:50:18.396139Z","iopub.status.idle":"2022-01-28T07:50:18.404541Z","shell.execute_reply.started":"2022-01-28T07:50:18.396098Z","shell.execute_reply":"2022-01-28T07:50:18.403552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np","metadata":{"execution":{"iopub.status.busy":"2022-01-28T07:52:17.45621Z","iopub.execute_input":"2022-01-28T07:52:17.45674Z","iopub.status.idle":"2022-01-28T07:52:17.460934Z","shell.execute_reply.started":"2022-01-28T07:52:17.456707Z","shell.execute_reply":"2022-01-28T07:52:17.45989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_input['data'][:]","metadata":{"execution":{"iopub.status.busy":"2022-01-28T07:52:38.080036Z","iopub.execute_input":"2022-01-28T07:52:38.080583Z","iopub.status.idle":"2022-01-28T07:52:38.586521Z","shell.execute_reply.started":"2022-01-28T07:52:38.080547Z","shell.execute_reply":"2022-01-28T07:52:38.585349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Run Custom loss","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\nimport h5py\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.utils import Sequence, to_categorical\nBASE_DIR = '../input/vr3d-pts-bordeau'\n# BASE_DIR = '../input/experience'\n# BASE_DIR = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(BASE_DIR)\nDATA_DIR = os.path.join(BASE_DIR, \"data/h5_data/\")\n\n\n\nclass PointCloudProvider(Sequence):\n    \"\"\"\n    Lazily load point clouds and annotations from filesystem and prepare it for model training.\n    \"\"\"\n\n    def __init__(self, mode='train', batch_size=32, n_classes=2, sample_size=4096, task=\"seg\"): #task=\"classification\"):\n        \"\"\"\n        Instantiate a data provider instance for point cloud data.\n        Args:\n            dataset: pandas DataFrame containing\n            n_classes: The number of different cthe index to the files (train or test set)\n            batch_size: the desired batch sizelasses (needed for one-hot encoding of labels)\n            sample_size: the amount of points to sample per instance.\n            task: string denoting the tasks for which the data is to be loaded. Either \"classification\" (default) or \"segmentaion\".\n        \"\"\"\n        self.datasets = {\n            'train': 'train_VDS_7.h5',\n            'test': 'test_VDS_7.h5'\n        }\n        self.mode = mode\n        self.batch_size = batch_size\n        self.n_classes = n_classes\n        self.sample_size = sample_size\n        self.task = task\n\n        self.indices = np.arange(0,len(h5py.File(self.datasets[self.mode], 'r')['data']), 1)\n        print( 'Indice = ', self.indices )\n\n        self.on_epoch_end()\n\n    def __len__(self):\n        return int(np.ceil((len(self.indices)/self.batch_size)))\n\n    def __getitem__(self, index):\n        \"\"\"Generate one batch of data.\"\"\"\n        batch_indices = self.indices[index * self.batch_size: (index + 1) * self.batch_size]\n        #print('Generated batch_indices = ', batch_indices.shape)\n        mask = np.zeros_like(self.indices).astype(np.bool)\n        mask[batch_indices] = True\n        X = h5py.File(self.datasets[self.mode], 'r')['data'][mask, ...]\n        #print('X',X.shape)\n        y = h5py.File(self.datasets[self.mode], 'r')['label'][mask, ...]\n        #print('y', y.shape, '\\n')\n\n        #return np.array(X), to_categorical(np.array(y), num_classes=self.n_classes)\n        #self.rotate_point_clouds(np.array(X))\n        return np.array(X), np.array(y)\n\n    def sample_random_points(self, pc):\n        r_idx = np.random.randint(pc.shape[1], size=self.sample_size)\n        return np.take(pc, r_idx, axis=1)\n\n    def on_epoch_end(self):\n        \"\"\"Shuffle training data, so batches are in different order\"\"\"\n        np.random.shuffle(self.indices)\n\n    def rotate_point_clouds(self, batch, rotation_angle_range=(-np.pi / 8, np.pi / 8)):\n        \"\"\"Rotate point cloud around y-axis (=up) by random angle\"\"\"\n        for b in range(batch.shape[0]):\n            phi = np.random.uniform(*rotation_angle_range)\n            c, s = np.cos(phi), np.sin(phi)\n            R = np.asarray([[c, 0, s],\n                           [0, 1, 0],\n                           [-s, 0, c]])\n            shape_pc = batch[b, ...]\n            batch[b, ...] = np.dot(shape_pc.reshape((-1, 3)), R)\n        return batch\n\n    def load_h5(self, h5_filename):\n        f = h5py.File(h5_filename, 'r')\n        data = f['data'][:]\n        label = f['label'][:]\n        return data, label\n\n    def load_data_file(self, filename):\n        if self.task == \"classification\":\n            return self.load_h5(filename)\n        else:\n            return self.load_h5_data_label_seg(filename)\n\n    def load_h5_data_label_seg(self, h5_filename):\n        f = h5py.File(h5_filename, 'r')\n        data = f['data'][:]\n        label = f['label'][:]\n        #seg = f['pid'][:]\n        return data, label  #, seg\n\n    @staticmethod\n    def initialize_dataset():\n        \"\"\"\n        Loads an index to all files and structures them.\n        :param data_directory: directory containing the data files\n        :param file_extension: extension of the data files\n        :return: pandas dataframe containing an index to all files and a label index,\n            mapping numerical label representations to label names.\n        \"\"\"\n\n        print(\"[Provider]: Creating Virtual Dataset\")\n\n        #train_index = os.path.join(BASE_DIR, 'train_files.txt')\n        #test_index = os.path.join(BASE_DIR, 'test_files.txt')\n        \n        train_index = '../input/vr3d-pts-bordeau/data/h5_data/train_files.txt'\n        test_index = '../input/vr3d-pts-bordeau/data/h5_data/test_files.txt'\n\n        # train_index = '../input/experience/data/h5_data/train_files.txt'\n        # test_index = '../input/experience/data/h5_data/test_files.txt'\n        \n        train_files = [line.rstrip() for line in open(train_index)]\n        test_files = [line.rstrip() for line in open(test_index)]\n        \n        def create_vds(files, prefix='train'):\n\n            print('\\nVDS creation...')\n            out_size = 0\n            for f in files:\n                out_size += h5py.File(DATA_DIR+f, 'r')['data'].shape[0]\n\n            print('Total h5 size : ', out_size) #(23585, 4096, 9)\n\n            # Assemble virtual dataset\n            point_layout = h5py.VirtualLayout(shape=(out_size, 4096, 7), dtype='uint8')\n            label_layout = h5py.VirtualLayout(shape=(out_size, 4096, 1), dtype='uint8')\n            print('\\nPoint VirtualLayout = ', point_layout.shape)\n            print('Label VirtualLayout = ', label_layout.shape)\n\n            for i, f in enumerate(files):\n                h5_path = DATA_DIR+f\n                size = len(h5py.File(h5_path, 'r')['data'])\n                start_idx = i*1000  # only the last chunk is smaller than 2048\n                end_idx = start_idx + size\n\n                print('\\nput in VirtualSource...')\n                vsource_points = h5py.VirtualSource(h5_path, 'data', shape=(size, 4096, 7), maxshape=(out_size, 4096, 7))\n                vsource_label = h5py.VirtualSource(h5_path, 'label', shape=(size, 4096, 1), maxshape=(out_size, 4096, 1))\n                print('vsource_points = ',vsource_points.shape,\n                      'vsource_label = ',vsource_label.shape)\n\n                point_layout[start_idx:end_idx, ...] = vsource_points\n                label_layout[start_idx:end_idx, ...] = vsource_label\n                print('point_layout = ', point_layout.shape,\n                      'label_layout = ', label_layout.shape)\n                print('In : layout[', start_idx,':',end_idx,'...]')\n\n            # Add virtual dataset to output file$\n\n            with h5py.File(\"{}_VDS_7.h5\".format(prefix), 'w', libver='latest') as f:\n                print('\\n\\tcreate_virtual_dataset for DATA...')\n                f.create_virtual_dataset('data', point_layout)\n                print('\\n\\t', point_layout.shape)\n\n                print('\\n\\tcreate_virtual_dataset for LABEL...')\n                f.create_virtual_dataset('label', label_layout)\n                print('\\n\\t', label_layout.shape)\n\n        create_vds(train_files, 'train')\n        create_vds(test_files, 'test')\n\n        print(\"[Provider] Created Virtual Dataset.\")\n\n\n#init = PointCloudProvider()\n#init.initialize_dataset()","metadata":{"execution":{"iopub.status.busy":"2022-04-08T10:47:42.259598Z","iopub.execute_input":"2022-04-08T10:47:42.259979Z","iopub.status.idle":"2022-04-08T10:47:46.961768Z","shell.execute_reply.started":"2022-04-08T10:47:42.259894Z","shell.execute_reply":"2022-04-08T10:47:46.960809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Pointcloud Baumann","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\nimport h5py\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.utils import Sequence, to_categorical\nBASE_DIR = '../input/vr3d-baumann'\n# BASE_DIR = '../input/experience'\n# BASE_DIR = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(BASE_DIR)\nDATA_DIR = os.path.join(BASE_DIR, \"data/h5_data/\")\n\n\n\nclass PointCloudProvider(Sequence):\n    \"\"\"\n    Lazily load point clouds and annotations from filesystem and prepare it for model training.\n    \"\"\"\n\n    def __init__(self, mode='train', batch_size=32, n_classes=2, sample_size=2048, task=\"seg\"): #task=\"classification\"):\n        \"\"\"\n        Instantiate a data provider instance for point cloud data.\n        Args:\n            dataset: pandas DataFrame containing\n            n_classes: The number of different cthe index to the files (train or test set)\n            batch_size: the desired batch sizelasses (needed for one-hot encoding of labels)\n            sample_size: the amount of points to sample per instance.\n            task: string denoting the tasks for which the data is to be loaded. Either \"classification\" (default) or \"segmentaion\".\n        \"\"\"\n        self.datasets = {\n            'train': 'train_VDS_7.h5',\n            'test': 'test_VDS_7.h5'\n        }\n        self.mode = mode\n        self.batch_size = batch_size\n        self.n_classes = n_classes\n        self.sample_size = sample_size\n        self.task = task\n\n        self.indices = np.arange(0,len(h5py.File(self.datasets[self.mode], 'r')['data']), 1)\n        print( 'Indice = ', self.indices )\n\n        self.on_epoch_end()\n\n    def __len__(self):\n        return int(np.ceil((len(self.indices)/self.batch_size)))\n\n    def __getitem__(self, index):\n        \"\"\"Generate one batch of data.\"\"\"\n        batch_indices = self.indices[index * self.batch_size: (index + 1) * self.batch_size]\n        #print('Generated batch_indices = ', batch_indices.shape)\n        mask = np.zeros_like(self.indices).astype(np.bool)\n        mask[batch_indices] = True\n        X = h5py.File(self.datasets[self.mode], 'r')['data'][mask, ...]\n        #print('X',X.shape)\n        y = h5py.File(self.datasets[self.mode], 'r')['label'][mask, ...]\n        #print('y', y.shape, '\\n')\n\n        #return np.array(X), to_categorical(np.array(y), num_classes=self.n_classes)\n        #self.rotate_point_clouds(np.array(X))\n        return np.array(X), np.array(y)\n\n    def sample_random_points(self, pc):\n        r_idx = np.random.randint(pc.shape[1], size=self.sample_size)\n        return np.take(pc, r_idx, axis=1)\n\n    def on_epoch_end(self):\n        \"\"\"Shuffle training data, so batches are in different order\"\"\"\n        np.random.shuffle(self.indices)\n\n    def rotate_point_clouds(self, batch, rotation_angle_range=(-np.pi / 8, np.pi / 8)):\n        \"\"\"Rotate point cloud around y-axis (=up) by random angle\"\"\"\n        for b in range(batch.shape[0]):\n            phi = np.random.uniform(*rotation_angle_range)\n            c, s = np.cos(phi), np.sin(phi)\n            R = np.asarray([[c, 0, s],\n                           [0, 1, 0],\n                           [-s, 0, c]])\n            shape_pc = batch[b, ...]\n            batch[b, ...] = np.dot(shape_pc.reshape((-1, 3)), R)\n        return batch\n\n    def load_h5(self, h5_filename):\n        f = h5py.File(h5_filename, 'r')\n        data = f['data'][:]\n        label = f['label'][:]\n        return data, label\n\n    def load_data_file(self, filename):\n        if self.task == \"classification\":\n            return self.load_h5(filename)\n        else:\n            return self.load_h5_data_label_seg(filename)\n\n    def load_h5_data_label_seg(self, h5_filename):\n        f = h5py.File(h5_filename, 'r')\n        data = f['data'][:]\n        label = f['label'][:]\n        #seg = f['pid'][:]\n        return data, label  #, seg\n\n    @staticmethod\n    def initialize_dataset():\n        \"\"\"\n        Loads an index to all files and structures them.\n        :param data_directory: directory containing the data files\n        :param file_extension: extension of the data files\n        :return: pandas dataframe containing an index to all files and a label index,\n            mapping numerical label representations to label names.\n        \"\"\"\n\n        print(\"[Provider]: Creating Virtual Dataset\")\n\n        #train_index = os.path.join(BASE_DIR, 'train_files.txt')\n        #test_index = os.path.join(BASE_DIR, 'test_files.txt')\n        \n        train_index = '../input/vr3d-baumann/data/h5_data/train_files.txt'\n        test_index = '../input/vr3d-baumann/data/h5_data/test_files.txt'\n\n        # train_index = '../input/experience/data/h5_data/train_files.txt'\n        # test_index = '../input/experience/data/h5_data/test_files.txt'\n        \n        train_files = [line.rstrip() for line in open(train_index)]\n        test_files = [line.rstrip() for line in open(test_index)]\n        \n        def create_vds(files, prefix='train'):\n\n            print('\\nVDS creation...')\n            out_size = 0\n            for f in files:\n                out_size += h5py.File(DATA_DIR+f, 'r')['data'].shape[0]\n\n            print('Total h5 size : ', out_size) #(23585, 4096, 9)\n\n            # Assemble virtual dataset\n            point_layout = h5py.VirtualLayout(shape=(out_size, 2048, 3), dtype='uint8')\n            label_layout = h5py.VirtualLayout(shape=(out_size, 2048, 1), dtype='uint8')\n            print('\\nPoint VirtualLayout = ', point_layout.shape)\n            print('Label VirtualLayout = ', label_layout.shape)\n\n            for i, f in enumerate(files):\n                h5_path = DATA_DIR+f\n                size = len(h5py.File(h5_path, 'r')['data'])\n                start_idx = i*1000  # only the last chunk is smaller than 2048\n                end_idx = start_idx + size\n\n                print('\\nput in VirtualSource...')\n                vsource_points = h5py.VirtualSource(h5_path, 'data', shape=(size, 2048, 3), maxshape=(out_size, 2048, 3))\n                vsource_label = h5py.VirtualSource(h5_path, 'label', shape=(size, 2048, 1), maxshape=(out_size, 2048, 1))\n                print('vsource_points = ',vsource_points.shape,\n                      'vsource_label = ',vsource_label.shape)\n\n                point_layout[start_idx:end_idx, ...] = vsource_points\n                label_layout[start_idx:end_idx, ...] = vsource_label\n                print('point_layout = ', point_layout.shape,\n                      'label_layout = ', label_layout.shape)\n                print('In : layout[', start_idx,':',end_idx,'...]')\n\n            # Add virtual dataset to output file$\n\n            with h5py.File(\"{}_VDS_7.h5\".format(prefix), 'w', libver='latest') as f:\n                print('\\n\\tcreate_virtual_dataset for DATA...')\n                f.create_virtual_dataset('data', point_layout)\n                print('\\n\\t', point_layout.shape)\n\n                print('\\n\\tcreate_virtual_dataset for LABEL...')\n                f.create_virtual_dataset('label', label_layout)\n                print('\\n\\t', label_layout.shape)\n\n        create_vds(train_files, 'train')\n        create_vds(test_files, 'test')\n\n        print(\"[Provider] Created Virtual Dataset.\")\n\n\n#init = PointCloudProvider()\n#init.initialize_dataset()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T06:17:17.003938Z","iopub.execute_input":"2022-02-03T06:17:17.004422Z","iopub.status.idle":"2022-02-03T06:17:22.809614Z","shell.execute_reply.started":"2022-02-03T06:17:17.00434Z","shell.execute_reply":"2022-02-03T06:17:22.808459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-11-09T08:24:38.695437Z","iopub.execute_input":"2021-11-09T08:24:38.69578Z","iopub.status.idle":"2021-11-09T08:24:38.71402Z","shell.execute_reply.started":"2021-11-09T08:24:38.695735Z","shell.execute_reply":"2021-11-09T08:24:38.713151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model with gridsearchcv","metadata":{}},{"cell_type":"code","source":"import argparse\nimport math\nimport h5py\nimport numpy as np\nimport tensorflow as tf\nimport keras.backend as K\nfrom tensorboard.plugins.hparams import api as hp\ntf.compat.v1.disable_eager_execution()\n#tf.compat.v1.enable_eager_execution()\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import f1_score\nimport socket\nimport importlib\nimport os\nimport sys\nimport datetime\nimport time \n\nBASE_DIR = '../input/vr3d-calibre-intensite'\nsys.path.append(BASE_DIR)\nsys.path.append(os.path.join(BASE_DIR, 'models'))\nsys.path.append(os.path.join(BASE_DIR, 'utils'))\nsys.path.append(os.path.join(BASE_DIR, 'logs'))\n#from provider2 import PointCloudProvider\nimport tf_util\n\nMODEL = importlib.import_module('pointnet_seg') \n\n#MAX_EPOCH = 15\n# Set the number of points to sample and batch size and parse the dataset.\nNUM_POINT = 4096  # help='Point Number [256/512/1024/2048/4096] [default: 1024]\nMAX_NUM_POINT = 4096 # default 4096\nBATCH_SIZE = 32\nNUM_CLASSES = 2 #13 #40\n\nBASE_LEARNING_RATE = 0.001 # √† descendre\nDECAY_STEP = 2000 # √† monter\nDECAY_RATE = 0.5 # √† desc \n\ndef get_learning_rate_schedule(instanc):\n    learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(\n        BASE_LEARNING_RATE*10**(-instanc),  # Initial learning rate\n        DECAY_STEP,          # Decay step.\n        DECAY_RATE-0.1*instanc,          # Decay rate.\n        staircase=True)\n    return learning_rate\n\ndef get_opt(instanc):\n    learning_rate = get_learning_rate_schedule(instanc)\n    # learning_rate = 0.0001\n    optimizer = tf.keras.optimizers.Adam(learning_rate)\n    return optimizer\n\n\ndef Dice_loss(y_true,y_pred): \n    #custom_loss=kb.square(y_actual-y_pred)\n    #https://arxiv.org/pdf/2003.07311.pdf (p3)\n   \n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.math.sigmoid(y_pred)\n    numerator = 2 * tf.reduce_sum(y_true * y_pred)\n    denominator = tf.reduce_sum(y_true + y_pred)\n\n    return 1 - numerator / denominator\n\ndef IOU_LOSS(y_true,y_pred):\n    tp = K.sum(K.round(K.clip(y_true*y_pred,0,1)))\n    tn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n    fp = K.sum(K.round(K.clip((1 - y_true) * y_pred, 0, 1)))\n    fn = K.sum(K.round(K.clip((y_true) * (1 - y_pred), 0, 1)))\n    return 1 -(tp )/(tp+fp+fn) # retourner en tant que loss\n\ndef pixel_accuracy(y_true,y_pred):\n    tp = K.sum(K.round(K.clip(y_true*y_pred,0,1)))\n    tn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n    fp = K.sum(K.round(K.clip((1 - y_true) * y_pred, 0, 1)))\n    fn = K.sum(K.round(K.clip((y_true) * (1 - y_pred), 0, 1)))\n    return (tp )/(tp+fp)\n\n\ndef model_m(model,instanc,MAX_EPOCH):\n    PointCloudProvider.initialize_dataset()\n    print('\\nput in generator...')\n        \n    generator_training = PointCloudProvider('train', BATCH_SIZE, n_classes=NUM_CLASSES, sample_size=MAX_NUM_POINT)\n    generator_validation = PointCloudProvider('test', BATCH_SIZE, n_classes=NUM_CLASSES, sample_size=MAX_NUM_POINT)\n        \n    print('\\ntraining...')\n    print(len(generator_training))\n    model.compile(optimizer=get_opt(instanc),\n                  loss = tf.keras.losses.sparse_categorical_crossentropy,\n                  metrics=[pixel_accuracy,IOU_LOSS,'accuracy'])\n    \n    log_dir = \"logs/logs_\"+str(instanc)+\"-\"+str(MAX_NUM_POINT)+\"/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1) \n    model.fit(generator_training,\n                        steps_per_epoch=len(generator_training),\n                        epochs=MAX_EPOCH,use_multiprocessing=False,callbacks =[tensorboard_callback]\n             )\n    model.save_weights(\"/kaggle/working/Trained_ep15_\"+str(instanc)+\"-\"+str(MAX_NUM_POINT)+\"_Down_sample_bordeau_elevation_filtre.h5\")\n    \n    print('*'*20,'resulta','*'*20)\n    print(f'Base_learning rate :{0.0001*10**(-instanc)}\\n')\n    print(f'Decay_rate:{0.5-0.1*instanc}\\n')\n    print(f'Batch_size:{MAX_NUM_POINT}\\n')\n    print(f'num_class:{instanc}\\n')\n    print('*'*20,'resulta','*'*20)\n    \ndef train():\n    with tf.device(\"gpu:0\"):\n        max_score = 0\n        for i in range(1,3):\n            for j in [18,20,22,23]:#batch_size\n                model = MODEL.get_model((None, 7), NUM_CLASSES) \n                model_m(model,i,j)\n                # result,instanc = model_metric(model,i,j)\n                # max_score = max(result,max_score)\n        # print(f'the best IoU score is: {max_score}')\nif __name__ == \"__main__\":\n    train()","metadata":{"execution":{"iopub.status.busy":"2022-02-16T12:47:48.201468Z","iopub.execute_input":"2022-02-16T12:47:48.201785Z","iopub.status.idle":"2022-02-16T14:52:38.385743Z","shell.execute_reply.started":"2022-02-16T12:47:48.201756Z","shell.execute_reply":"2022-02-16T14:52:38.384794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow_addons as tfa\nimport argparse\nimport math\n\nimport h5py\nimport numpy as np\nimport tensorflow as tf\nfrom keras import backend as K\nfrom tensorflow.keras import backend as K, Model\nfrom tensorflow.keras.layers import Input, Lambda, concatenate\ntf.compat.v1.disable_eager_execution()\nfrom keras.models import Sequential\n#tf.compat.v1.enable_eager_execution()\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import f1_score\nimport socket\nimport importlib\nimport os\nimport sys\nimport datetime\nimport time \n\n\nBASE_DIR ='../input/down-sample-bordeau-elevation-filtre'\nsys.path.append(BASE_DIR)\nsys.path.append(os.path.join(BASE_DIR, 'models'))\nsys.path.append(os.path.join(BASE_DIR, 'utils'))\nsys.path.append(os.path.join(BASE_DIR, 'logs'))\nimport tf_util\n\nMODEL = importlib.import_module('pointnet_seg') # import network module\n\nMAX_EPOCH = 5\nNUM_POINT = 4096  # help='Point Number [256/512/1024/2048/4096] [default: 1024]\nMAX_NUM_POINT = 4096\nBATCH_SIZE = 32\nNUM_CLASSES = 2 #2 #13 #40\n\nBASE_LEARNING_RATE = 0.001\nDECAY_STEP = 20000\nDECAY_RATE = 0.3\n\ndef get_learning_rate_schedule():\n    learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(\n        BASE_LEARNING_RATE,  # Initial learning rate\n        DECAY_STEP,          # Decay step.\n        DECAY_RATE,          # Decay rate.\n        staircase=True)\n    return learning_rate\n\ndef Dice_loss(y_true,y_pred): \n    # y_true = tf.cast(y_true, tf.float32)\n    # y_pred = tf.math.sigmoid(y_pred)\n    numerator = 2 * tf.reduce_sum(y_true * y_pred)\n    denominator = tf.reduce_sum(y_true + y_pred)\n    return 1 - numerator / denominator\n\ndef DiceLoss(y_true, y_pred):\n        # y_true = tf.cast(y_true, tf.float32)\n    # y_pred = tf.math.sigmoid(y_pred)\n    numerator = 2 * K.sum(y_true * y_pred)\n    denominator = K.sum(y_true + y_pred)\n    return 1 - numerator / denominator\n    \n\ndef pixel_accuracy(y_true,y_pred):\n    tp = K.sum(K.round(K.clip(y_true*y_pred,0,1)))\n    tn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n    fp = K.sum(K.round(K.clip((1 - y_true) * y_pred, 0, 1)))\n    fn = K.sum(K.round(K.clip((y_true) * (1 - y_pred), 0, 1)))\n    return (tp )/(tp+fp)  #(tp )/(tp+fp) pourcentage de la valeur positive pr√®dit sont vraie\n\ndef recall(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall_keras = true_positives / (possible_positives + K.epsilon())\n    return recall_keras\n\ndef precision(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision_keras = true_positives / (predicted_positives + K.epsilon())\n    return precision_keras\n\ndef f1(y_true, y_pred):\n    p = precision(y_true, y_pred)\n    r = recall(y_true, y_pred)\n    return 2 * ((p * r) / (p + r + K.epsilon()))\n\ndef IOU(y_true,y_pred):\n    tp = K.sum(K.round(K.clip(y_true*y_pred,0,1)))\n    tn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n    fp = K.sum(K.round(K.clip((1 - y_true) * y_pred, 0, 1)))\n    fn = K.sum(K.round(K.clip((y_true) * (1 - y_pred), 0, 1)))\n    return 1 -(tp )/(tp+fp+fn) # retourner en tant que loss\n\ndef train():\n    with tf.device(\"gpu:0\"):\n            new_model = get_model((None, 7), NUM_CLASSES)\n            #model = MODEL.get_model((4096, 7), NUM_CLASSES)\n            new_model.summary()\n            #inputs = Input((4096,7), name='Input_cloud')\n            # Reshape the output model\n            #new_model = Sequential()\n            #new_model.add(model)\n            #new_model.add(tf.keras.layers.Flatten())\n            #new_model.add(tf.keras.layers.Dense(2,activation='relu'))\n            #new_model.add(tf.keras.layers.Reshape((-1,2)))\n            #x = tf.keras.layers.Flatten()\n            #outputs = tf.keras.layers.Dense(2, activation='relu')(x)# inputs and outputs\n            #new_model = tf.keras.Model(inputs=model, outputs=outputs, name=\"test_for_shap\")\n            #new_model.summary()\n            \n            \n            \n            learning_rate = get_learning_rate_schedule()\n            # optimizer = tf.keras.optimizers.Adam(0.01)\n            optimizer = tf.keras.optimizers.Adam(learning_rate)\n        \n            # initialize Dataset\n            PointCloudProvider.initialize_dataset()\n        \n            print('\\nput in generator...')\n            generator_training = PointCloudProvider('train', BATCH_SIZE, n_classes=NUM_CLASSES, sample_size=MAX_NUM_POINT)\n            generator_validation = PointCloudProvider('test', BATCH_SIZE, n_classes=NUM_CLASSES, sample_size=MAX_NUM_POINT)\n            print('\\ntraining...')\n            \n            log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y-%m-%d-%H:%M:%S\")\n            tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n            \n            loss = tf.keras.losses.sparse_categorical_crossentropy\n            new_model.compile(optimizer=optimizer,\n                              loss = loss,\n                              #loss = DiceLoss,\n                              #metrics = [pixel_accuracy,IOU,'accuracy']\n                              # metrics=[\"sparse_categorical_accuracy\"]\n                              metrics=['accuracy']\n                            )\n            pred = new_model.predict(generator_training) \n            print(pred,pred.shape)\n            new_model.save_weights(\"/kaggle/working/Trained_epSHAP_\"+datetime.datetime.now().strftime(\"%Y-%m-%d-%H:%M:%S\")+\"_Down_sample_bordeau_elevation_filtre.h5\")\n            new_model.fit(generator_training, \n                      #validation_data=generator_validation,\n                      steps_per_epoch=len(generator_training),\n                      #validation_steps=len(generator_validation),\n                      epochs=MAX_EPOCH,use_multiprocessing=False,\n                      callbacks = [tensorboard_callback]\n                     )\n            \n            print('\\nsave model...')\n            \n            print('\\nFinished...')\n            \n            \nif __name__ == \"__main__\":\n    train()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow_addons as tfa\nimport argparse\nimport math\nimport h5py\nimport numpy as np\nimport tensorflow as tf\nfrom keras import backend as K\ntf.compat.v1.disable_eager_execution()\n#tf.compat.v1.enable_eager_execution()\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import f1_score\nimport socket\nimport importlib\nimport os\nimport sys\nimport datetime\nimport time \n\n\nBASE_DIR ='../input/vr3d-baumann'\nsys.path.append(BASE_DIR)\nsys.path.append(os.path.join(BASE_DIR, 'models'))\nsys.path.append(os.path.join(BASE_DIR, 'utils'))\nsys.path.append(os.path.join(BASE_DIR, 'logs'))\nimport tf_util\n\nMODEL = importlib.import_module('pointnet_seg') # import network module\n\nMAX_EPOCH = 30\nNUM_POINT = 4096  # help='Point Number [256/512/1024/2048/4096] [default: 1024]\nMAX_NUM_POINT = 2048\nBATCH_SIZE = 32\nNUM_CLASSES = 2 #2 #13 #40\n\nBASE_LEARNING_RATE = 0.00001\nDECAY_STEP = 2000\nDECAY_RATE = 0.3\n\ndef get_learning_rate_schedule():\n    learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(\n        BASE_LEARNING_RATE,  # Initial learning rate\n        DECAY_STEP,          # Decay step.\n        DECAY_RATE,          # Decay rate.\n        staircase=True)\n    return learning_rate\n\ndef Dice_loss(y_true,y_pred): \n    # y_true = tf.cast(y_true, tf.float32)\n    # y_pred = tf.math.sigmoid(y_pred)\n    numerator = 2 * tf.reduce_sum(y_true * y_pred)\n    denominator = tf.reduce_sum(y_true + y_pred)\n    return 1 - numerator / denominator\n\ndef DiceLoss(y_true, y_pred):\n        # y_true = tf.cast(y_true, tf.float32)\n    # y_pred = tf.math.sigmoid(y_pred)\n    numerator = 2 * K.sum(y_true * y_pred)\n    denominator = K.sum(y_true + y_pred)\n    return 1 - numerator / denominator\n    \n\ndef pixel_accuracy(y_true,y_pred):\n    tp = K.sum(K.round(K.clip(y_true*y_pred,0,1)))\n    tn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n    fp = K.sum(K.round(K.clip((1 - y_true) * y_pred, 0, 1)))\n    fn = K.sum(K.round(K.clip((y_true) * (1 - y_pred), 0, 1)))\n    return (tp )/(tp+fp)  #(tp )/(tp+fp) pourcentage de la valeur positive pr√®dit sont vraie\n\ndef recall(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall_keras = true_positives / (possible_positives + K.epsilon())\n    return recall_keras\n\ndef precision(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision_keras = true_positives / (predicted_positives + K.epsilon())\n    return precision_keras\n\ndef f1(y_true, y_pred):\n    p = precision(y_true, y_pred)\n    r = recall(y_true, y_pred)\n    return 2 * ((p * r) / (p + r + K.epsilon()))\n\ndef IOU(y_true,y_pred):\n    tp = K.sum(K.round(K.clip(y_true*y_pred,0,1)))\n    tn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n    fp = K.sum(K.round(K.clip((1 - y_true) * y_pred, 0, 1)))\n    fn = K.sum(K.round(K.clip((y_true) * (1 - y_pred), 0, 1)))\n    return 1 -(tp )/(tp+fp+fn) # retourner en tant que loss\n\ndef train():\n    with tf.device(\"gpu:0\"):\n            model = MODEL.get_model((None, 3), NUM_CLASSES)\n            model.summary()\n            learning_rate = get_learning_rate_schedule()\n            # optimizer = tf.keras.optimizers.Adam(0.01)\n            optimizer = tf.keras.optimizers.Adam(learning_rate)\n        \n            # initialize Dataset\n            PointCloudProvider.initialize_dataset()\n        \n            print('\\nput in generator...')\n            generator_training = PointCloudProvider('train', BATCH_SIZE, n_classes=NUM_CLASSES, sample_size=MAX_NUM_POINT)\n            generator_validation = PointCloudProvider('test', BATCH_SIZE, n_classes=NUM_CLASSES, sample_size=MAX_NUM_POINT)\n            print('\\ntraining...')\n            \n            log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y-%m-%d-%H:%M:%S\")\n            tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n            \n            loss = tf.keras.losses.sparse_categorical_crossentropy\n            model.compile(optimizer=optimizer,\n                          loss=loss,\n                          #metrics=['categorical_accuracy']\n                          #loss = DiceLoss,\n                          metrics = [pixel_accuracy,IOU,'accuracy']\n                          #metrics=[\"sparse_categorical_accuracy\"]\n                          #metrics=['accuracy']\n                            )\n\n            model.fit(generator_training, \n                      #validation_data=generator_validation,\n                      steps_per_epoch=len(generator_training),\n                      #validation_steps=len(generator_validation),\n                      epochs=MAX_EPOCH,use_multiprocessing=False,\n                      callbacks = [tensorboard_callback]\n                     )\n            \n            print('\\nsave model...')\n            model.save_weights(\"/kaggle/working/Trained_ep30BA_\"+datetime.datetime.now().strftime(\"%Y-%m-%d-%H:%M:%S\")+\"_Down_sample_bordeau_elevation_filtre.h5\")\n            print('\\nFinished...')\n            \n            \nif __name__ == \"__main__\":\n    train()","metadata":{},"execution_count":null,"outputs":[]}]}